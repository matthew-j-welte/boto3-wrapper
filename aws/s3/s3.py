import os
import time
import urllib

from logging import Logger
from pathlib import Path
from typing import Tuple, Union

from botocore.exceptions import ClientError

from aws import AWS
from aws.exceptions import ConstructorAccessDenied, InvalidUrlException, InvalidS3Type

_logger = Logger(__name__)

class S3(AWS):
    """Wrapper class for boto3 s3 client and resource objects"""
    __create_key = object()
    def __init__(self, create_key: object, url: str, bucket: str, key: str):
        if create_key != S3.__create_key:
            err_msg = "Can only create an S3 Object using the from_url and from_bucket_and_key classmethods"
            raise ConstructorAccessDenied(err_msg)
        self.url = url
        self.bucket = bucket
        self.key = key

    @property
    def name(self) -> str:
        """The name of the aws resource that will be passed into boto3.client()"""
        return "s3"

    @property
    def src_dict(self) -> dict:
        """Generates a dictionary commonly used for boto3 s3 copy operations"""
        return {
            "Bucket": self.bucket,
            "Key": self.key
        }

    @classmethod
    def from_url(cls, url: str) -> Union[S3, None]:
        """Factory method for creating an S3 object generated by an s3 url"""
        if cls.valid_url(url):
            bucket, key = cls.split_url(url)
            return S3(cls.__create_key, url, bucket, key)
        return None

    @classmethod
    def from_bucket_and_key(cls, bucket: str, key: str = None) -> Union[S3, None]:
        """Factory method for creating an S3 object from a bucket/key pair"""
        if key is None:
            key = ""
        url = cls.join_url(bucket, key)
        if cls.valid_url(url):
            return S3(cls.__create_key, url, bucket, key)
        return None

    @staticmethod
    def valid_url(url) -> bool:
        """
        Tests if the s3 url provided matches the schema expected by s3.
        This function does not test if the url exists in s3
        """
        parsed = urllib.parse.urlparse(url)
        scheme = parsed.scheme
        bucket = parsed.netloc
        if scheme == "s3" and bucket:
            return True
        raise InvalidUrlException("The schema provided did not generate a valid s3 url")

    @staticmethod
    def split_url(url: str) -> Tuple[str, str]:
        """Accepts an s3 url and returns the bucket/key pair associated with it"""
        parsed = urllib.parse.urlparse(url)
        bucket = parsed.netloc
        path = parsed.path if parsed.path[0] != "/" else parsed.path[1:]
        return bucket, path

    @staticmethod
    def join_url(bucket: str, key: str):
        """Joins a bucket/key pair into a valid s3 url"""
        return f"s3://{bucket}/{key}"

    def create(self, run_async: bool = False, **kwargs) -> bool:
        """
        Generic create function. 
        Creates the path specified in the class - If no key was provided creates the bucket
        """
        try:
            if self.exists():
                _logger.warning(f"{self.url} already exists.. skipping creation")
                return False
            if not self.key:
                self.__create_bucket(**kwargs)
            self.__create_path(**kwargs)
            return True
        except ClientError:
            _logger.exception("Client error raised when creating resource.")
            return False

    def wait_for_arrival(self, timeout: int = 5) -> bool:
        """
        Function used to wait for some creation/upload/copy operation to complete.
        Continually checks the location specified in the S3 object until it arrives
        This will commonly be used when operations are ran async
        """
        elapsed = 0
        while elapsed < timeout:
            if self.exists():
                return True
            time.sleep(1)
            elapsed += 1
        return False

    def exists(self, **kwargs) -> bool:
        """The bucket or object specified by this class exists"""
        bucket, key = self.split_url(self.url)
        if key:
            self.resource.head_object(Bucket=bucket, Key=key, **kwargs)
        else:
            self.resource.head_bucket(Bucket=bucket, **kwargs)
        return True

    def copy_from_local(self, local_path: str, rename: str = None, run_async: bool = False, **kwargs) -> bool:
        """Copies a file from local to the path specified in s3
        Args:
            local_path (str): The local path we want to copy to
            rename (str, optional): Used if a user wants to rename the local file
                                    after copying it to s3. Defaults to None.
            run_async (bool, optional): True if run this in background. Defaults to False.
        """
        local_path = Path(local_path)
        if not local_path.exists():
            return False
        basename = rename if rename else local_path.name
        upload_location = os.path.join(self.key, basename)
        try:
            self.client.upload_file(str(local_path), self.bucket, upload_location, **kwargs)
            return True
        except ClientError:
            _logger.exception("Unable to copy local file to S3")
            return False

    def move_from_local(self, local_path: str, rename: str = None, run_async: bool = False, **kwargs) -> None:
        """Moves a file from local to s3
        Args:
            local_path (str): The local path we want to copy to
            rename (str, optional): Used if a user wants to rename the local file
                                    after moving it to s3. Defaults to None.
            run_async (bool, optional): True if run this in background. Defaults to False.
        """
        self.copy_from_local(local_path, rename=rename, **kwargs)
        self.__delete_from_local(local_path)

    def download_to(self, local_path: str, rename: str = None, run_async: bool = False, **kwargs) -> None:
        """Downloads the file specified by this obj to some local path
        Args:
            local_path (str): The local path we want to download to
            rename (str, optional): Used if a user wants to rename the local file
                                    after downloading it from s3. Defaults to None.
            run_async (bool, optional): True if run this in background. Defaults to False.
        """
        local_path = Path(local_path)
        key_basename = Path(self.key).name
        if local_path.is_dir:
            local_path.joinpath(rename if rename else key_basename)
        elif local_path.is_file:
            local_path = local_path.joinpath(*local_path.parts[:-1], rename if rename else key_basename)
        elif Path(*local_path.parts[:-1]).is_dir:
            pass
        else:
            raise FileNotFoundError("The local file path/directory can not be found")

        self.resource.Bucket(self.bucket).download_file(self.key, local_path, **kwargs)

    def copy_from_s3(self, s3_source: S3, run_async: bool = False, **kwargs) -> None:
        """Copy from passed in S3 location to location specified in this obj"""
        self.__copy(s3_source, self, **kwargs)

    def move_from_s3(self, s3_source: S3, run_async: bool = False, **kwargs) -> None:
        """Move from passed in S3 location to location specified in this obj"""
        self.copy_from_s3(s3_source, **kwargs)
        s3_source.delete()

    def copy_to_s3(self, s3_destination: S3, run_async: bool = False, **kwargs) -> None:
        """Copy to passed in S3 location from location specified in this obj"""
        self.__copy(self, s3_destination, **kwargs)

    def move_to_s3(self, s3_destination: S3, run_async: bool = False, **kwargs) -> None:
        """Move to passed in S3 location from location specified in this obj"""
        self.copy_to_s3(s3_destination, **kwargs)
        s3_destination.delete()

    def delete(self, run_async: bool = False) -> None:
        """Deletes the location specified by this obj"""
        self.client.delete_object(Bucket=self.bucket, Key=self.key)

    def __create_bucket(self, **kwargs) -> None:
        self.client.create_bucket(Bucket=self.bucket, **kwargs)

    def __create_path(self, **kwargs) -> None:
        self.client.put_object(Bucket=self.bucket, Key=self.key, **kwargs)

    @staticmethod
    def __delete_from_local(local_path: str) -> None:
        try:
            Path(local_path).unlink()
        except FileNotFoundError:
            _logger.warning("Local file has already been deleted/moved.. skipping deletion")

    def __copy(self, source: S3, destination: S3, **kwargs) -> None:
        if not isinstance(source, S3) or not isinstance(destination, S3):
            raise InvalidS3Type("Must pass in Object of type S3")
        try:
            self.resource.Bucket(destination.bucket).copy(source.src_dict, destination.key, **kwargs)
        except ClientError:
            _logger.exception("Unable to perform s3 to s3 copy")
